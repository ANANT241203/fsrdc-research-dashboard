{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3EEsz9YWcemk",
        "outputId": "136af169-85f4-4102-958b-f77b845a4fc4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import requests\n",
        "import re\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Part 1: Merge Title/DOI pairs from all group CSV files\n",
        "    dfs = []\n",
        "    title_candidates = [\"ProjectTitle\", \"Title\", \"title\"]\n",
        "    doi_candidates = [\"DOI\", \"doi\"]\n",
        "    for path in glob.glob(\"group*.csv\"):\n",
        "        df = pd.read_csv(path, dtype=str)\n",
        "        title_col = next((c for c in title_candidates if c in df.columns), None)\n",
        "        doi_col = next((c for c in doi_candidates if c in df.columns), None)\n",
        "        if doi_col is None:\n",
        "            print(f\"Skipping {path}: DOI column not found\")\n",
        "            continue\n",
        "        sub = pd.DataFrame({\n",
        "            \"Title\": df[title_col] if title_col else pd.NA,\n",
        "            \"DOI\": df[doi_col]\n",
        "        }).dropna(subset=[\"DOI\", \"Title\"])\n",
        "        dfs.append(sub)\n",
        "\n",
        "    if dfs:\n",
        "        combined = pd.concat(dfs, ignore_index=True).drop_duplicates(subset=[\"DOI\", \"Title\"])  # Deduplicate\n",
        "    else:\n",
        "        combined = pd.DataFrame(columns=[\"Title\", \"DOI\"])\n",
        "\n",
        "    combined.to_csv(\"merged_titles_dois.csv\", index=False)\n",
        "    print(f\"Merged {combined.shape[0]} records\")\n",
        "\n",
        "    # Part 2: Enrich with project metadata\n",
        "    df = pd.read_csv(\"merged_titles_dois.csv\", dtype=str)\n",
        "    meta = pd.read_excel(\"ProjectsAllMetadata.xlsx\", sheet_name=\"All Metadata\", dtype=str)\n",
        "    meta_sub = meta[[\"Proj ID\", \"Status\", \"Title\", \"RDC\", \"Start Year\", \"End Year\", \"PI\"]].rename(columns={\n",
        "        \"Proj ID\": \"ProjID\",\n",
        "        \"Status\": \"ProjectStatus\",\n",
        "        \"Title\": \"Title_meta\",\n",
        "        \"RDC\": \"ProjectRDC\",\n",
        "        \"Start Year\": \"ProjectYearStarted\",\n",
        "        \"End Year\": \"ProjectYearEnded\",\n",
        "        \"PI\": \"ProjectPI\"\n",
        "    })\n",
        "    enriched = df.merge(\n",
        "        meta_sub,\n",
        "        left_on=\"Title\",\n",
        "        right_on=\"Title_meta\",\n",
        "        how=\"left\"\n",
        "    )\n",
        "    final_cols = [\n",
        "        \"Title\", \"DOI\",\n",
        "        \"ProjID\", \"ProjectStatus\", \"ProjectRDC\",\n",
        "        \"ProjectYearStarted\", \"ProjectYearEnded\", \"ProjectPI\"\n",
        "    ]\n",
        "    result = enriched[final_cols].copy()\n",
        "    result[\"DOI\"] = result[\"DOI\"].str.strip().str.replace(\n",
        "        r\"^https?://(?:dx\\.)?doi\\.org/\", \"\", regex=True\n",
        "    )\n",
        "    result.to_csv(\"merged_with_project_metadata.csv\", index=False)\n",
        "    print(f\"Total records: {len(result)}, Projects matched: {result['ProjID'].notna().sum()}\")\n",
        "\n",
        "    # Part 3: Fetch metadata from Crossref and OpenAlex\n",
        "    df = pd.read_csv(\"merged_with_project_metadata.csv\", dtype=str)\n",
        "\n",
        "    def fetch_crossref(doi: str) -> dict:\n",
        "        url = f\"https://api.crossref.org/works/{doi}\"\n",
        "        resp = requests.get(url, timeout=10)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()[\"message\"]\n",
        "        title = (data.get(\"title\") or [\"\"])[0]\n",
        "        authors = data.get(\"author\", [])\n",
        "        author_strs = [f\"{a.get('family','')}, {a.get('given','')}\" for a in authors]\n",
        "        biblio = \"; \".join(author_strs) + \\\n",
        "            f\" ({data.get('issued',{}).get('date-parts',[[None]])[0][0]}). {title}. \" \\\n",
        "            f\"{(data.get('container-title') or [''])[0]}\"\n",
        "        type_map = {\n",
        "            \"journal-article\": \"JA\",\n",
        "            \"book-chapter\": \"BC\",\n",
        "            \"dissertation\": \"DI\",\n",
        "            \"report\": \"RE\",\n",
        "            \"posted-content\": \"WP\"\n",
        "        }\n",
        "        out_type = type_map.get(data.get(\"type\", \"\"), \"\")\n",
        "        out_status = \"PB\" if data.get(\"status\", \"\") == \"published\" else \"FC\"\n",
        "        issued = data.get(\"issued\", {}).get(\"date-parts\", [[None, None]])[0]\n",
        "        return {\n",
        "            \"OutputTitle\": title,\n",
        "            \"OutputBiblio\": biblio,\n",
        "            \"OutputType\": out_type,\n",
        "            \"OutputStatus\": out_status,\n",
        "            \"OutputVenue\": (data.get(\"container-title\") or [\"\"])[0],\n",
        "            \"OutputYear\": issued[0],\n",
        "            \"OutputMonth\": issued[1] if len(issued) > 1 else None,\n",
        "            \"OutputVolume\": data.get(\"volume\", \"\"),\n",
        "            \"OutputNumber\": data.get(\"issue\", \"\"),\n",
        "            \"OutputPages\": data.get(\"page\", \"\")\n",
        "        }\n",
        "\n",
        "    def fetch_openalex(doi: str) -> dict:\n",
        "        url = f\"https://api.openalex.org/works/{doi}\"\n",
        "        resp = requests.get(url, timeout=10)\n",
        "        if resp.status_code != 200:\n",
        "            return {}\n",
        "        data = resp.json().get(\"data\", {})\n",
        "        auths = [a[\"author\"][\"display_name\"] for a in data.get(\"authorships\", [])]\n",
        "        return {\"Authors\": \", \".join(auths)} if auths else {}\n",
        "\n",
        "    def scrape_all_metadata(doi: str) -> dict:\n",
        "        cr_meta = fetch_crossref(doi)\n",
        "        oa_meta = fetch_openalex(doi)\n",
        "        if oa_meta.get(\"Authors\"):\n",
        "            cr_meta[\"Authors\"] = oa_meta[\"Authors\"]\n",
        "        return cr_meta\n",
        "\n",
        "    dois = df[\"DOI\"].dropna().unique().tolist()\n",
        "    records = []\n",
        "    for doi in dois:\n",
        "        try:\n",
        "            print(f\"Fetching metadata for DOI: {doi}\")\n",
        "            meta = scrape_all_metadata(doi)\n",
        "            meta[\"DOI\"] = doi\n",
        "            records.append(meta)\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching DOI={doi}: {e}\")\n",
        "\n",
        "    meta_df = pd.DataFrame.from_records(records)\n",
        "    cols = [\"DOI\"] + [c for c in meta_df.columns if c != \"DOI\"]\n",
        "    meta_df = meta_df[cols]\n",
        "    merged = df.merge(meta_df, on=\"DOI\", how=\"left\")\n",
        "    merged.to_csv(\"with_all_Output_metadata.csv\", index=False)\n",
        "    print(\"Saved intermediate metadata to: with_all_Output_metadata.csv\")\n",
        "    merged = pd.read_csv(\"with_all_Output_metadata.csv\")\n",
        "    merged = merged.dropna(subset=[\"OutputBiblio\"])\n",
        "    merged.to_csv(\"with_all_Output_metadata_cleaned.csv\", index=False)\n",
        "    # Part 4: Clean and filter records\n",
        "    df_clean = pd.read_csv(\"with_all_Output_metadata_cleaned.csv\", dtype=str)\n",
        "    mask = pd.Series(True, index=df_clean.index)\n",
        "    mask &= df_clean[\"OutputPages\"].apply(\n",
        "        lambda x: bool(re.fullmatch(r\"\\d+(-\\d+)?\", x.strip()))\n",
        "        if isinstance(x, str) and x.strip() else True\n",
        "    )\n",
        "    for col in df_clean.columns:\n",
        "        df_clean[col] = df_clean[col].astype(str)\n",
        "        mask &= ~df_clean[col].apply(\n",
        "            lambda txt: bool(re.search(r\"[\\u4e00-\\u9fff]\", txt))\n",
        "        )\n",
        "    mask &= (df_clean.shape[1] <= 18)\n",
        "    df_final = df_clean[mask]\n",
        "    df_final.to_csv(\"with_all_Output_metadata_final.csv\", index=False)\n",
        "\n",
        "    # Part 5: Fetch authors again for final set\n",
        "    df_auth = pd.read_csv(\"with_all_Output_metadata_final.csv\", dtype=str)\n",
        "    df_auth['author'] = df_auth['DOI'].apply(lambda d: fetch_crossref(d).get('OutputBiblio', ''))\n",
        "    df_auth.to_csv(\"with_all_Output_metadata_with_authors.csv\", index=False)\n",
        "    print(\"Authors added to final set and saved to: with_all_Output_metadata_with_authors.csv\")\n",
        "\n",
        "    # Part 6: Match researchers\n",
        "    df_authors = pd.read_csv(\"with_all_Output_metadata_with_authors.csv\", dtype=str)\n",
        "    df_meta_r = pd.read_excel(\"ProjectsAllMetadata.xlsx\", sheet_name=\"Researchers\", dtype=str)\n",
        "    researchers = df_meta_r[['Researcher','Proj ID','Status','Title','RDC','Start Year','End Year','PI']]\n",
        "\n",
        "    def match_researchers_to_project(author_str):\n",
        "        if not isinstance(author_str, str) or not author_str.strip():\n",
        "            return []\n",
        "        matched = []\n",
        "        for name in author_str.split('; '):\n",
        "            mask = researchers['Researcher'].str.lower().str.find(name.lower()) >= 0\n",
        "            subset = researchers[mask]\n",
        "            if not subset.empty:\n",
        "                matched.append(subset.iloc[0])\n",
        "        return matched\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        match_data = list(tqdm(\n",
        "            executor.map(match_researchers_to_project, df_authors['author']),\n",
        "            total=len(df_authors), desc=\"Matching researchers\"\n",
        "        ))\n",
        "\n",
        "    for col in ['Proj ID','Status','Title','RDC','Start Year','End Year','PI']:\n",
        "        df_authors[col] = [\n",
        "            \", \".join(str(r[col]) for r in rows) if rows else \"\"\n",
        "            for rows in match_data\n",
        "        ]\n",
        "    df_authors.to_csv(\"with_all_Output_metadata_with_authors_and_projects.csv\", index=False)\n",
        "    print(\"Researcher fields appended and saved to: with_all_Output_metadata_with_authors_and_projects.csv\")\n",
        "\n",
        "    # Part 7: Final cleanup\n",
        "    df_out = pd.read_csv(\"with_all_Output_metadata_with_authors_and_projects.csv\", dtype=str)\n",
        "    drop_cols = ['Title','ProjID','ProjectStatus','ProjectRDC','ProjectYearEnded','ProjectPI']\n",
        "    df_out.drop(columns=drop_cols, inplace=True)\n",
        "    df_out.dropna(subset=['Proj ID'], inplace=True)\n",
        "    df_out.to_csv(\"processed_data.csv\", index=False)\n",
        "    df_out = pd.read_csv(\"processed_data.csv\", dtype=str)\n",
        "    df_out.dropna(subset=['Proj ID'], inplace=True)\n",
        "    df_out.drop(columns=['ProjectYearStarted'], inplace=True)\n",
        "    df_out.to_csv(\"processed_data_cleaned.csv\", index=False)\n",
        "    df_out.to_excel(\"processed_data_cleaned.xlsx\", index=False)\n",
        "    print(df_out.head())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OanddAILL3O0",
        "outputId": "bdf207fc-21c1-4c9e-bda8-cc0e7c6a46a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authors added to final set and saved to: with_all_Output_metadata_with_authors.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Matching researchers: 100%|██████████| 13857/13857 [1:04:00<00:00,  3.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Researcher fields appended and saved to: with_all_Output_metadata_with_authors_and_projects.csv\n",
            "                          DOI  \\\n",
            "0        10.32469/10355/91014   \n",
            "1              10.17226/22659   \n",
            "2  10.25300/misq/2017/41:1.03   \n",
            "3              10.17226/25660   \n",
            "4    10.1787/9789264308114-en   \n",
            "\n",
            "                                         OutputTitle  \\\n",
            "0  Bayesian unit-level modeling of non-Gaussian s...   \n",
            "1               Freight Trip Generation and Land Use   \n",
            "2  Digital Innovation Management: Reinventing Inn...   \n",
            "3     Impacts of Policy-Induced Freight Modal Shifts   \n",
            "4                          Financing Climate Futures   \n",
            "\n",
            "                                        OutputBiblio OutputType OutputStatus  \\\n",
            "0  , ; Parker, Paul Arthur (None). Bayesian unit-...         DI           FC   \n",
            "1  Holguín-Veras, José; Jaller, Miguel; Sanchez-D...        NaN           FC   \n",
            "2  , ; Nambisan, Satish; Lyytinen, Kalle; , ; Maj...         JA           FC   \n",
            "3  Institute, Rensselaer Polytechnic; , ; , ; , ;...        NaN           FC   \n",
            "4     , ; , ; ,  (2018). Financing Climate Futures.         NaN           FC   \n",
            "\n",
            "     OutputVenue OutputYear OutputMonth OutputVolume OutputNumber OutputPages  \\\n",
            "0            NaN        NaN         NaN          NaN          NaN         NaN   \n",
            "1            NaN     2012.0         1.0          NaN          NaN         NaN   \n",
            "2  MIS Quarterly     2017.0         1.0           41            1     223-238   \n",
            "3            NaN     2019.0        12.0          NaN          NaN         NaN   \n",
            "4            NaN     2018.0        11.0          NaN          NaN         NaN   \n",
            "\n",
            "                                              author        Proj ID  \\\n",
            "0  , ; Parker, Paul Arthur (None). Bayesian unit-...            926   \n",
            "1  Holguín-Veras, José; Jaller, Miguel; Sanchez-D...  926, 926, 926   \n",
            "2  , ; Nambisan, Satish; Lyytinen, Kalle; , ; Maj...  926, 926, 926   \n",
            "3  Institute, Rensselaer Polytechnic; , ; , ; , ;...  926, 926, 926   \n",
            "4     , ; , ; ,  (2018). Financing Climate Futures.        926, 926   \n",
            "\n",
            "                            Status                     RDC        Start Year  \\\n",
            "0                        Completed                  Baruch              2014   \n",
            "1  Completed, Completed, Completed  Baruch, Baruch, Baruch  2014, 2014, 2014   \n",
            "2  Completed, Completed, Completed  Baruch, Baruch, Baruch  2014, 2014, 2014   \n",
            "3  Completed, Completed, Completed  Baruch, Baruch, Baruch  2014, 2014, 2014   \n",
            "4             Completed, Completed          Baruch, Baruch        2014, 2014   \n",
            "\n",
            "           End Year                                        PI  \n",
            "0              2017                              Paul T Scott  \n",
            "1  2017, 2017, 2017  Paul T Scott, Paul T Scott, Paul T Scott  \n",
            "2  2017, 2017, 2017  Paul T Scott, Paul T Scott, Paul T Scott  \n",
            "3  2017, 2017, 2017  Paul T Scott, Paul T Scott, Paul T Scott  \n",
            "4        2017, 2017                Paul T Scott, Paul T Scott  \n"
          ]
        }
      ],
      "source": [
        "def fetch_crossref(doi: str) -> dict:\n",
        "    url = f\"https://api.crossref.org/works/{doi}\"\n",
        "    resp = requests.get(url, timeout=20)\n",
        "    resp.raise_for_status()\n",
        "    data = resp.json()[\"message\"]\n",
        "    title = (data.get(\"title\") or [\"\"])[0]\n",
        "    authors = data.get(\"author\", [])\n",
        "    author_strs = [f\"{a.get('family','')}, {a.get('given','')}\" for a in authors]\n",
        "    biblio = \"; \".join(author_strs) + \\\n",
        "        f\" ({data.get('issued',{}).get('date-parts',[[None]])[0][0]}). {title}. \" \\\n",
        "        f\"{(data.get('container-title') or [''])[0]}\"\n",
        "    type_map = {\n",
        "        \"journal-article\": \"JA\",\n",
        "        \"book-chapter\": \"BC\",\n",
        "        \"dissertation\": \"DI\",\n",
        "        \"report\": \"RE\",\n",
        "        \"posted-content\": \"WP\"\n",
        "    }\n",
        "    out_type = type_map.get(data.get(\"type\", \"\"), \"\")\n",
        "    out_status = \"PB\" if data.get(\"status\", \"\") == \"published\" else \"FC\"\n",
        "    issued = data.get(\"issued\", {}).get(\"date-parts\", [[None, None]])[0]\n",
        "    return {\n",
        "        \"OutputTitle\": title,\n",
        "        \"OutputBiblio\": biblio,\n",
        "        \"OutputType\": out_type,\n",
        "        \"OutputStatus\": out_status,\n",
        "        \"OutputVenue\": (data.get(\"container-title\") or [\"\"])[0],\n",
        "        \"OutputYear\": issued[0],\n",
        "        \"OutputMonth\": issued[1] if len(issued) > 1 else None,\n",
        "        \"OutputVolume\": data.get(\"volume\", \"\"),\n",
        "        \"OutputNumber\": data.get(\"issue\", \"\"),\n",
        "        \"OutputPages\": data.get(\"page\", \"\")\n",
        "    }\n",
        "\n",
        "def fetch_openalex(doi: str) -> dict:\n",
        "    url = f\"https://api.openalex.org/works/{doi}\"\n",
        "    resp = requests.get(url, timeout=20)\n",
        "    if resp.status_code != 200:\n",
        "        return {}\n",
        "    data = resp.json().get(\"data\", {})\n",
        "    auths = [a[\"author\"][\"display_name\"] for a in data.get(\"authorships\", [])]\n",
        "    return {\"Authors\": \", \".join(auths)} if auths else {}\n",
        "\n",
        "def scrape_all_metadata(doi: str) -> dict:\n",
        "    cr_meta = fetch_crossref(doi)\n",
        "    oa_meta = fetch_openalex(doi)\n",
        "    if oa_meta.get(\"Authors\"):\n",
        "        cr_meta[\"Authors\"] = oa_meta[\"Authors\"]\n",
        "    return cr_meta\n",
        "\n",
        "merged = pd.read_csv(\"with_all_Output_metadata.csv\")\n",
        "merged = merged.dropna(subset=[\"OutputBiblio\"])\n",
        "merged.to_csv(\"with_all_Output_metadata_cleaned.csv\", index=False)\n",
        "\n",
        "# Part 4: Clean and filter records\n",
        "df_clean = pd.read_csv(\"with_all_Output_metadata_cleaned.csv\", dtype=str)\n",
        "mask = pd.Series(True, index=df_clean.index)\n",
        "mask &= df_clean[\"OutputPages\"].apply(\n",
        "    lambda x: bool(re.fullmatch(r\"\\d+(-\\d+)?\", x.strip()))\n",
        "    if isinstance(x, str) and x.strip() else True\n",
        ")\n",
        "for col in df_clean.columns:\n",
        "    df_clean[col] = df_clean[col].astype(str)\n",
        "    mask &= ~df_clean[col].apply(\n",
        "        lambda txt: bool(re.search(r\"[\\u4e00-\\u9fff]\", txt))\n",
        "    )\n",
        "mask &= (df_clean.shape[1] <= 18)\n",
        "df_final = df_clean[mask]\n",
        "df_final.to_csv(\"with_all_Output_metadata_final.csv\", index=False)\n",
        "\n",
        "# Part 5: Fetch authors again for final set\n",
        "df_auth = pd.read_csv(\"with_all_Output_metadata_final.csv\", dtype=str)\n",
        "df_auth['author'] = df_auth['DOI'].apply(lambda d: fetch_crossref(d).get('OutputBiblio', ''))\n",
        "df_auth.to_csv(\"with_all_Output_metadata_with_authors.csv\", index=False)\n",
        "print(\"Authors added to final set and saved to: with_all_Output_metadata_with_authors.csv\")\n",
        "\n",
        "# Part 6: Match researchers\n",
        "df_authors = pd.read_csv(\"with_all_Output_metadata_with_authors.csv\", dtype=str)\n",
        "df_meta_r = pd.read_excel(\"ProjectsAllMetadata.xlsx\", sheet_name=\"Researchers\", dtype=str)\n",
        "researchers = df_meta_r[['Researcher','Proj ID','Status','Title','RDC','Start Year','End Year','PI']]\n",
        "\n",
        "def match_researchers_to_project(author_str):\n",
        "    if not isinstance(author_str, str) or not author_str.strip():\n",
        "        return []\n",
        "    matched = []\n",
        "    for name in author_str.split('; '):\n",
        "        mask = researchers['Researcher'].str.lower().str.find(name.lower()) >= 0\n",
        "        subset = researchers[mask]\n",
        "        if not subset.empty:\n",
        "            matched.append(subset.iloc[0])\n",
        "    return matched\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "    match_data = list(tqdm(\n",
        "        executor.map(match_researchers_to_project, df_authors['author']),\n",
        "        total=len(df_authors), desc=\"Matching researchers\"\n",
        "    ))\n",
        "\n",
        "for col in ['Proj ID','Status','Title','RDC','Start Year','End Year','PI']:\n",
        "    df_authors[col] = [\n",
        "        \", \".join(str(r[col]) for r in rows) if rows else \"\"\n",
        "        for rows in match_data\n",
        "    ]\n",
        "df_authors.to_csv(\"with_all_Output_metadata_with_authors_and_projects.csv\", index=False)\n",
        "print(\"Researcher fields appended and saved to: with_all_Output_metadata_with_authors_and_projects.csv\")\n",
        "\n",
        "# Part 7: Final cleanup\n",
        "df_out = pd.read_csv(\"with_all_Output_metadata_with_authors_and_projects.csv\", dtype=str)\n",
        "drop_cols = ['Title','ProjID','ProjectStatus','ProjectRDC','ProjectYearEnded','ProjectPI']\n",
        "df_out.drop(columns=drop_cols, inplace=True)\n",
        "df_out.dropna(subset=['Proj ID'], inplace=True)\n",
        "df_out.to_csv(\"processed_data.csv\", index=False)\n",
        "df_out = pd.read_csv(\"processed_data.csv\", dtype=str)\n",
        "df_out.dropna(subset=['Proj ID'], inplace=True)\n",
        "df_out.drop(columns=['ProjectYearStarted'], inplace=True)\n",
        "df_out.to_csv(\"processed_data_cleaned.csv\", index=False)\n",
        "df_out.to_excel(\"processed_data_cleaned.xlsx\", index=False)\n",
        "print(df_out.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSfHfPRNL6Bn",
        "outputId": "2bee029b-27fc-49ff-cb41-86d7c18e2ca3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import requests\n",
        "from datetime import datetime\n",
        "import re\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "\n",
        "def main():\n",
        "    def fetch_crossref(doi: str) -> dict:\n",
        "        url = f\"https://api.crossref.org/works/{doi}\"\n",
        "        resp = requests.get(url, timeout=10)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()[\"message\"]\n",
        "        output_title = data.get(\"title\", [\"\"])[0]\n",
        "        authors = data.get(\"author\", [])\n",
        "        author_strs = [f\"{a.get('family','')}, {a.get('given','')}\" for a in authors]\n",
        "        output_biblio = \"; \".join(author_strs) + \\\n",
        "            f\" ({data.get('issued',{}).get('date-parts',[[None]])[0][0]}). \" \\\n",
        "            f\"{output_title}. {data.get('container-title',[''])[0]}\"\n",
        "        type_map = {\n",
        "            \"journal-article\": \"JA\",\n",
        "            \"book-chapter\":     \"BC\",\n",
        "            \"dissertation\":     \"DI\",\n",
        "            \"report\":           \"RE\",\n",
        "            \"posted-content\":   \"WP\",\n",
        "        }\n",
        "        cr_type      = data.get(\"type\", \"\")\n",
        "        output_type  = type_map.get(cr_type, \"\")\n",
        "        status       = data.get(\"status\",\"\")\n",
        "        output_status = \"PB\" if status==\"published\" else \"FC\"\n",
        "        issued = data.get(\"issued\",{}).get(\"date-parts\",[[None,None]])\n",
        "        return {\n",
        "            \"OutputTitle\":   output_title,\n",
        "            \"OutputBiblio\":  output_biblio,\n",
        "            \"OutputType\":    output_type,\n",
        "            \"OutputStatus\":  output_status,\n",
        "            \"OutputVenue\":   data.get(\"container-title\",[\"\"])[0],\n",
        "            \"OutputYear\":    issued[0][0],\n",
        "            \"OutputMonth\":   (issued[0][1] if len(issued[0])>1 else None),\n",
        "            \"OutputVolume\":  data.get(\"volume\",\"\"),\n",
        "            \"OutputNumber\":  data.get(\"issue\",\"\"),\n",
        "            \"OutputPages\":   data.get(\"page\",\"\"),\n",
        "        }\n",
        "\n",
        "    def fetch_openalex(doi: str) -> dict:\n",
        "        url = f\"https://api.openalex.org/works/{doi}\"\n",
        "        resp = requests.get(url, timeout=10)\n",
        "        if resp.status_code != 200:\n",
        "            return {}\n",
        "        data = resp.json().get(\"data\",{})\n",
        "        auths = [a[\"author\"][\"display_name\"] for a in data.get(\"authorships\",[])]\n",
        "        return {\"Authors\": \", \".join(auths)}\n",
        "\n",
        "    def scrape_all_metadata(doi: str) -> dict:\n",
        "        meta = fetch_crossref(doi)\n",
        "        oa   = fetch_openalex(doi)\n",
        "        if oa.get(\"Authors\"):\n",
        "            meta[\"Authors\"] = oa[\"Authors\"]\n",
        "        return meta\n",
        "\n",
        "    # 2. 定义校验函数\n",
        "    def valid_year(x):\n",
        "        # 年份要是 4 位数字，例如 2022\n",
        "        return bool(isinstance(x, str) and re.fullmatch(r\"\\d{4}\", x.strip()))\n",
        "\n",
        "    def valid_month(x):\n",
        "        # 月份为空 OR 1 到 12 之间\n",
        "        if pd.isna(x) or not x.strip():\n",
        "            return True\n",
        "        return bool(re.fullmatch(r\"[1-9]|1[0-2]\", x.strip()))\n",
        "\n",
        "    def valid_pages(x):\n",
        "        # 页码为空 OR “数字” 或 “数字-数字”\n",
        "        if pd.isna(x) or not x.strip():\n",
        "            return True\n",
        "        return bool(re.fullmatch(r\"\\d+(-\\d+)?\", x.strip()))\n",
        "\n",
        "    def valid_doi(x):\n",
        "        # DOI 格式粗校验：数字.数字/任意\n",
        "        return bool(isinstance(x, str) and re.fullmatch(r\"\\d+\\.\\d+/.+\", x.strip()))\n",
        "\n",
        "    # 3. 定义函数检测中文字符\n",
        "    def contains_chinese(text):\n",
        "        \"\"\"如果文本中包含中文字符，返回 True\"\"\"\n",
        "        if isinstance(text, str):\n",
        "            return bool(re.search(r'[\\u4e00-\\u9fff]', text))\n",
        "        return False\n",
        "\n",
        "    # 定义一个函数来爬取 DOI 的作者\n",
        "    def fetch_author_from_doi(doi):\n",
        "        url = f\"https://api.crossref.org/works/{doi}\"\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            data = response.json().get('message', {})\n",
        "            authors = data.get('author', [])\n",
        "\n",
        "            author_names = []\n",
        "            for a in authors:\n",
        "                # 检查是否存在 family 和 given 字段\n",
        "                family = a.get('family', '')\n",
        "                given = a.get('given', '')\n",
        "                if family and given:\n",
        "                    author_names.append(f\"{given} {family}\")\n",
        "                elif family:\n",
        "                    author_names.append(f\"{family}\")\n",
        "                elif given:\n",
        "                    author_names.append(f\"{given}\")\n",
        "\n",
        "            author_str = \"; \".join(author_names) if author_names else \"\"\n",
        "\n",
        "            # 打印爬取到的作者\n",
        "            print(f\"DOI: {doi} -> Authors: {author_str}\")\n",
        "\n",
        "            return author_str\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"DOI: {doi} -> Error: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    # 1. 定义候选列名\n",
        "    title_candidates = [\"ProjectTitle\", \"Title\", \"title\"]\n",
        "    doi_candidates   = [\"DOI\", \"doi\"]\n",
        "\n",
        "    # 2. 读取每个 CSV，提取 Title+DOI 并重命名\n",
        "    dfs = []\n",
        "    for path in glob.glob(\"group*.csv\"):\n",
        "        df = pd.read_csv(path, dtype=str)\n",
        "\n",
        "        # 找到第一个存在的 Title 列\n",
        "        title_col = next((c for c in title_candidates if c in df.columns), None)\n",
        "        # 找到第一个存在的 DOI 列\n",
        "        doi_col   = next((c for c in doi_candidates   if c in df.columns), None)\n",
        "\n",
        "        # 如果没有 DOI 列，则跳过这个文件\n",
        "        if doi_col is None:\n",
        "            print(f\"跳过 {path}，原因：未找到 DOI 列\")\n",
        "            continue\n",
        "\n",
        "        # 取出并重命名\n",
        "        sub = pd.DataFrame({\n",
        "            \"Title\": df[title_col] if title_col else pd.NA,\n",
        "            \"DOI\":   df[doi_col]\n",
        "        })\n",
        "        sub = sub.dropna(subset=[\"DOI\", \"Title\"])\n",
        "        dfs.append(sub)\n",
        "\n",
        "    # 3. 合并并去重\n",
        "    if dfs:\n",
        "        combined = pd.concat(dfs, ignore_index=True)\n",
        "        combined = combined.drop_duplicates(subset=[\"DOI\", \"Title\"], keep=\"first\")\n",
        "    else:\n",
        "        combined = pd.DataFrame(columns=[\"Title\",\"DOI\"])\n",
        "\n",
        "    # 4. 保存结果\n",
        "    combined.to_csv(\"merged_titles_dois.csv\", index=False)\n",
        "\n",
        "    # 输出查看\n",
        "    print(combined.shape[0])\n",
        "\n",
        "    # 1. 读取只含 Title/DOI 的清洗表\n",
        "    df = pd.read_csv(\"merged_titles_dois.csv\", dtype=str)\n",
        "\n",
        "    # 2. 读取项目元数据（All metadata 表）\n",
        "    meta = pd.read_excel(\n",
        "        \"ProjectsAllMetadata.xlsx\",\n",
        "        sheet_name=\"All Metadata\",\n",
        "        dtype=str\n",
        "    )\n",
        "\n",
        "    # 3. 从 meta 表选出原始列，并重命名以便合并后列名一致\n",
        "    meta_sub = meta[[\n",
        "        \"Proj ID\", \"Status\", \"Title\", \"RDC\", \"Start Year\", \"End Year\", \"PI\"\n",
        "    ]].rename(columns={\n",
        "        \"Proj ID\":       \"ProjID\",\n",
        "        \"Status\":        \"ProjectStatus\",\n",
        "        \"Title\":         \"Title_meta\",\n",
        "        \"RDC\":           \"ProjectRDC\",\n",
        "        \"Start Year\":    \"ProjectYearStarted\",\n",
        "        \"End Year\":      \"ProjectYearEnded\",\n",
        "        \"PI\":            \"ProjectPI\"\n",
        "    })\n",
        "\n",
        "    # 4. 与主表按 Title 做左连接\n",
        "    enriched = df.merge(\n",
        "        meta_sub,\n",
        "        left_on  = \"Title\",\n",
        "        right_on = \"Title_meta\",\n",
        "        how      = \"left\"\n",
        "    )\n",
        "\n",
        "    # 5. 构建最终表格列序\n",
        "    final_cols = [\n",
        "        \"Title\", \"DOI\",\n",
        "        \"ProjID\", \"ProjectStatus\", \"ProjectRDC\",\n",
        "        \"ProjectYearStarted\", \"ProjectYearEnded\", \"ProjectPI\"\n",
        "    ]\n",
        "    result = enriched[final_cols]\n",
        "    result[\"DOI\"] = (\n",
        "        result[\"DOI\"]\n",
        "        .str.strip()\n",
        "        .str.replace(r\"^https?://(?:dx\\.)?doi\\.org/\", \"\", regex=True)\n",
        "    )\n",
        "    # 6. 保存\n",
        "    result.to_csv(\"merged_with_project_metadata.csv\", index=False)\n",
        "\n",
        "    print(f\"总记录数：{len(result)}，匹配到项目元数据的记录数：{result['ProjID'].notna().sum()}\")\n",
        "\n",
        "    # 1. 读取原始 CSV\n",
        "    df = pd.read_csv(\"merged_with_project_metadata.csv\", dtype=str)\n",
        "\n",
        "    # 2. 对唯一的 DOI 列表去重，避免重复调用\n",
        "    dois = df[\"DOI\"].dropna().unique().tolist()\n",
        "\n",
        "    # 3. 批量爬取并收集结果\n",
        "    records = []\n",
        "    for doi in dois:\n",
        "        try:\n",
        "            print(doi)\n",
        "            meta = scrape_all_metadata(doi)\n",
        "            meta[\"DOI\"] = doi\n",
        "            records.append(meta)\n",
        "        except Exception as e:\n",
        "            print(f\"爬取 DOI={doi} 时出错：{e}\")\n",
        "\n",
        "    # 4. 构造成 DataFrame\n",
        "    meta_df = pd.DataFrame.from_records(records)\n",
        "    # 确保列顺序：DOI 在最前\n",
        "    cols = [\"DOI\"] + [c for c in meta_df.columns if c!=\"DOI\"]\n",
        "    meta_df = meta_df[cols]\n",
        "\n",
        "    # 5. 与原表按 DOI 合并\n",
        "    merged = df.merge(meta_df, on=\"DOI\", how=\"left\")\n",
        "\n",
        "    # 6. 保存结果\n",
        "    merged.to_csv(\"with_all_Output_metadata.csv\", index=False)\n",
        "    print(\"完成，输出保存在 with_all_Output_metadata.csv\")\n",
        "\n",
        "    merged = merged.dropna(subset=[\"OutputBiblio\"])\n",
        "    merged.to_csv(\"with_all_Output_metadata_cleaned.csv\", index=False)\n",
        "\n",
        "    df = pd.read_csv(\"with_all_Output_metadata_cleaned.csv\", dtype=str)\n",
        "    # 4. 逐项过滤\n",
        "    mask = pd.Series(True, index=df.index)\n",
        "    mask &= df[\"OutputPages\"].apply(valid_pages)\n",
        "    # 检查所有列是否包含中文字符\n",
        "    for column in df.columns:\n",
        "        # 确保该列是字符串类型\n",
        "        df[column] = df[column].astype(str)\n",
        "        mask &= ~df[column].apply(contains_chinese)  # 如果该列含中文字符，则该行被删除\n",
        "    # 5. 删除列数超过18的行\n",
        "    mask &= (df.shape[1] <= 18)\n",
        "\n",
        "    # 5. 应用过滤并查看被删除的数量\n",
        "    removed = len(df) - mask.sum()\n",
        "    print(f\"将删除 {removed} 行格式不合规或包含中文字符的数据。\")\n",
        "\n",
        "    # 6. 获取清洗后的数据\n",
        "    df_clean2 = df[mask]\n",
        "\n",
        "    # 7. 保存结果\n",
        "    df_clean2.to_csv(\"with_all_Output_metadata_final.csv\", index=False)\n",
        "    df = pd.read_csv(\"with_all_Output_metadata_final.csv\", dtype=str)\n",
        "\n",
        "    # 爬取每个 DOI 的作者并打印\n",
        "    df['author'] = df['DOI'].apply(fetch_author_from_doi)\n",
        "\n",
        "    # 保存更新后的文件\n",
        "    df.to_csv(\"with_all_Output_metadata_with_authors.csv\", index=False)\n",
        "\n",
        "    print(\"添加作者信息完成，文件已保存为 'with_all_Output_metadata_with_authors.csv'\")\n",
        "    # 重新读取上传的两个文件\n",
        "    df_authors = pd.read_csv(\"with_all_Output_metadata_with_authors.csv\", dtype=str)\n",
        "    df_metadata = pd.read_excel(\"ProjectsAllMetadata.xlsx\", sheet_name=\"Researchers\", dtype=str)\n",
        "\n",
        "    # 提取 Researcher 表中的 Researcher 列和其他需要的列\n",
        "    researchers = df_metadata[['Researcher', 'Proj ID', 'Status', 'Title', 'RDC', 'Start Year', 'End Year', 'PI']]\n",
        "\n",
        "    # 定义一个函数来匹配作者并添加相关数据\n",
        "    def match_researchers_to_project(authors_str):\n",
        "        # 如果作者为空或NaN，跳过\n",
        "        if not isinstance(authors_str, str) or not authors_str.strip():\n",
        "            return []\n",
        "\n",
        "        # 提取每个作者的姓名（例如：姓, 名）\n",
        "        authors_list = authors_str.split('; ')\n",
        "        matched_row = []\n",
        "\n",
        "        for author in authors_list:\n",
        "            # 查找 Researcher 表中是否有匹配的作者姓名\n",
        "            matched = researchers[researchers['Researcher'].str.contains(author, case=False, na=False)]\n",
        "            if not matched.empty:\n",
        "                matched_row.append(matched.iloc[0])  # 取第一个匹配的行\n",
        "\n",
        "        return matched_row\n",
        "\n",
        "    # 使用 ThreadPoolExecutor 并行化 DOI 爬取，并且添加进度条\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        # 用 tqdm 包装 iterable 来添加进度条\n",
        "        matching_data = list(tqdm(executor.map(match_researchers_to_project, df_authors['author']),\n",
        "                                total=len(df_authors), desc=\"Processing authors\"))\n",
        "\n",
        "    # 将匹配到的数据添加到 df_authors 中\n",
        "    proj_columns = ['Proj ID', 'Status', 'Title', 'RDC', 'Start Year', 'End Year', 'PI']\n",
        "    for col in proj_columns:\n",
        "        df_authors[col] = [\", \".join([str(row[col]) for row in rows]) if rows else \"\" for rows in matching_data]\n",
        "\n",
        "    # 保存结果\n",
        "    df_authors.to_csv(\"with_all_Output_metadata_with_authors_and_projects.csv\", index=False)\n",
        "\n",
        "    print(\"数据处理完成，文件已保存为 'with_all_Output_metadata_with_authors_and_projects.csv'\")\n",
        "    # 1. 删除指定列\n",
        "    columns_to_drop = ['Title', 'ProjID', 'ProjectStatus', 'ProjectRDC', 'ProjectYearEnded', 'ProjectPI']\n",
        "    df_authors.drop(columns=columns_to_drop, inplace=True)\n",
        "\n",
        "    # 2. 删除 'Proj ID' 列为空的行\n",
        "    df_authors.dropna(subset=['Proj ID'], inplace=True)\n",
        "    df_authors.to_csv(\"processed_data.csv\", index=False)\n",
        "\n",
        "    # 读取上传的文件\n",
        "    df = pd.read_csv(\"processed_data.csv\", dtype=str)\n",
        "\n",
        "    # 删除 'Proj ID' 列为空的行\n",
        "    df.dropna(subset=['Proj ID'], inplace=True)\n",
        "    df.drop(columns=['ProjectYearStarted'], inplace=True)\n",
        "    # 保存处理后的文件\n",
        "    df.to_csv(\"processed_data_cleaned.csv\", index=False)\n",
        "    df.to_excel(\"processed_data_cleaned.xlsx\", index=False)\n",
        "    df.head()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puF-13EkyWSi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
